geom_col(width = .1) +
# geom_point(size = 1) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, size = 7)
)
quanteda_texts_dfm %>%
textstat_frequency(groups = artist_name) %>%
filter(feature == "baby") %>%
ggplot(aes(x = group, y = frequency, label = feature)) +
geom_col(width = .1) +
# geom_point(size = 1) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, size = 7)
)
quanteda_texts_dfm %>%
textstat_frequency(groups = artist_name) %>%
filter(feature == "culo") %>%
ggplot(aes(x = group, y = frequency, label = feature)) +
geom_col(width = .1) +
# geom_point(size = 1) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, size = 7)
)
quanteda_texts_dfm %>%
textstat_frequency(groups = artist_name) %>%
filter(feature == "perra") %>%
ggplot(aes(x = group, y = frequency, label = feature)) +
geom_col(width = .1) +
# geom_point(size = 1) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, size = 7)
)
quanteda_texts_dfm %>%
textstat_frequency(groups = artist_name) %>%
filter(feature == "bitch") %>%
ggplot(aes(x = group, y = frequency, label = feature)) +
geom_col(width = .1) +
# geom_point(size = 1) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, size = 7)
)
kwic_test <- quanteda_texts_tok %>%
kwic(pattern =  "ell*", window = 10)
head(kwic_test, 10)
kwic_test <- quanteda_texts_tok %>%
kwic(pattern =  c("ella*","mujer*"),
window = 5)
head(kwic_test, 10)
tstat_lexdiv <- quanteda_texts_dfm %>%
textstat_lexdiv()
View(tstat_lexdiv)
tstat_dist <- quanteda_texts_dfm %>%
textstat_dist() %>%
as.dist()
tstat_dist %>%
hclust() %>%
plot(xlab = "Distance", ylab = NULL)
# call the packages
library(quanteda)
library(quanteda.textstats)
# call the packages
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(tidyverse)
library(readtext)
library(topicmodels)
library(ggplot2)
# load the tokenized texts
load("quanteda_texts_tok.Rdata")
list(names(quanteda_texts_tok))
quanteda_texts_dfm <- dfm(quanteda_texts_tok)
readtext("useless_words.txt")
gender_dfm <- tokens(quanteda_texts_tok, remove_punct = TRUE) %>%
tokens_remove(stopwords("en")) %>%
tokens_remove(stopwords("es")) %>%
tokens_remove(readtext("useless_words.txt")$text) %>%
tokens_group(groups = is_male) %>%
dfm()
# Calculate keyness and determine gender as target group
result_keyness <- textstat_keyness(gender_dfm)
# Plot estimated word keyness
textplot_keyness(result_keyness)
gender_dfm <- tokens(quanteda_texts_tok, remove_punct = TRUE) %>%
tokens_remove(stopwords("en")) %>%
tokens_remove(stopwords("es")) %>%
tokens_remove(readtext("useless_words.txt")$text) %>%
tokens_group(groups = is_male) %>%
dfm()
# Calculate keyness and determine gender as target group
result_keyness <- textstat_keyness(gender_dfm)
# Plot estimated word keyness
textplot_keyness(result_keyness)
# call the packages
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(tidyverse)
library(readtext)
library(topicmodels)
library(ggplot2)
# load the tokenized texts
load("quanteda_texts_tok.Rdata")
list(names(quanteda_texts_tok))
quanteda_texts_dfm <- dfm(quanteda_texts_tok)
gender_dfm <- tokens(quanteda_texts_tok, remove_punct = TRUE) %>%
tokens_remove(stopwords("en")) %>%
tokens_remove(stopwords("es")) %>%
tokens_remove(readtext("useless_words.txt")$text) %>%
tokens_group(groups = is_male) %>%
dfm()
# Calculate keyness and determine gender as target group
result_keyness <- textstat_keyness(gender_dfm)
tstat_key <- quanteda_texts_dfm %>%
textstat_keyness(target = grepl("Gaskell", quanteda_texts_dfm@docvars$docname_))
gender_dfm <- tokens(quanteda_texts_tok, remove_punct = TRUE) %>%
tokens_remove(stopwords("en")) %>%
tokens_remove(stopwords("es")) %>%
tokens_remove(readtext("useless_words.txt")$text) %>%
tokens_group(groups = is_male) %>%
dfm()
# Calculate keyness and determine gender as target group
result_keyness <- textstat_keyness(gender_dfm)
# Plot estimated word keyness
textplot_keyness(result_keyness)
readtext("useless_words.txt")$text
readtext("useless_words.txt", encoding = "UTF-8")$text
readtext("useless_words.txt", encoding = "UTF-8", dvsep = "\n")$text
readtext("useless_words.txt", encoding = "UTF-8", dv_sep = "\n")$text
readtext("useless_words.txt", encoding = "UTF-8")
readtext("useless_words.txt", encoding = "UTF-8")$text %>% tidyr::unnest_lines()
readtext("useless_words.txt", encoding = "UTF-8")$text %>% tidytext::unnest_lines()
readtext("useless_words.txt", encoding = "UTF-8") %>%
tidytext::unnest_lines(input = "text", output = "words")
readtext("useless_words.txt", encoding = "UTF-8") %>%
tidytext::unnest_lines(input = "text", output = "words")
gender_dfm <- tokens(quanteda_texts_tok, remove_punct = TRUE) %>%
tokens_remove(stopwords("en")) %>%
tokens_remove(stopwords("es")) %>%
tokens_remove(useless_words$words) %>%
tokens_group(groups = is_male) %>%
dfm()
useless_words <- readtext("useless_words.txt", encoding = "UTF-8") %>%
tidytext::unnest_lines(input = "text", output = "words")
gender_dfm <- tokens(quanteda_texts_tok, remove_punct = TRUE) %>%
tokens_remove(stopwords("en")) %>%
tokens_remove(stopwords("es")) %>%
tokens_remove(useless_words$words) %>%
tokens_group(groups = is_male) %>%
dfm()
# Calculate keyness and determine gender as target group
result_keyness <- textstat_keyness(gender_dfm)
# Plot estimated word keyness
textplot_keyness(result_keyness)
tstat_key <- quanteda_texts_dfm %>%
textstat_keyness(target = grepl("Young Miko", quanteda_texts_dfm@docvars$docname_))
textplot_keyness(tstat_key)
library(seededlda)
library(topicmodels)
tmod_lda <- textmodel_lda(quanteda_texts_dfm, k = 5)
save(tmod_lda, file = "tmod_lda.RData")
terms(tmod_lda, 10)
# Convert to matrix
dtm <- as.matrix(gender_dfm)
# Fit LDA model
set.seed(123)
lda <- LDA(dtm, k = 5, control = list(seed = 1234))
# Visualize the topics
topics <- tidy(lda, matrix = "beta")
library(tidytext)
# Visualize the topics
topics <- tidy(lda, matrix = "beta")
install.packages("reshape2")
# Visualize the topics
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ggplot(top_terms, aes(x = reorder(term, beta), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
coord_flip() +
labs(x = NULL, y = "Beta") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
clean_dfm <- tokens(quanteda_texts_tok, remove_punct = TRUE) %>%
tokens_remove(stopwords("en")) %>%
tokens_remove(stopwords("es")) %>%
tokens_remove(useless_words$words) %>%
dfm()
# Convert to matrix
dtm <- as.matrix(clean_dfm)
# Fit LDA model
set.seed(123)
lda <- LDA(dtm, k = 5, control = list(seed = 1234))
# Visualize the topics
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ggplot(top_terms, aes(x = reorder(term, beta), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
coord_flip() +
labs(x = NULL, y = "Beta") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
useless_words <- readtext("useless_words.txt", encoding = "UTF-8") %>%
tidytext::unnest_lines(input = "text", output = "words")
clean_dfm <- tokens(quanteda_texts_tok, remove_punct = TRUE) %>%
tokens_remove(stopwords("en")) %>%
tokens_remove(stopwords("es")) %>%
tokens_remove(useless_words$words) %>%
dfm()
# Convert to matrix
dtm <- as.matrix(clean_dfm)
# Fit LDA model
set.seed(123)
lda <- LDA(dtm, k = 5, control = list(seed = 1234))
# Visualize the topics
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ggplot(top_terms, aes(x = reorder(term, beta), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
coord_flip() +
labs(x = NULL, y = "Beta") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
# Extract topic distribution for each document
doc_topics <- tidy(lda, matrix = "gamma", document_names = rownames(dtm))
# Plot the distribution of topics across documents
doc_topics %>%
ggplot(aes(x = topic, y = gamma, fill = factor(topic))) +
geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
geom_boxplot(width = 0)
# load packages
library("mallet")
# load packages
library("mallet")
library("ggwordcloud")
library("reshape2")
# Define variables
num_topics <- 5 # number of topics
len_split <- 10000 # length of the split texts (they will be the actual documents to work on)
n_iterations <- 200 # number of iterations
# Prepare the corpus
tm_corpus <- list()
my_texts <- character()
file_list <- list.files("corpus_all", full.names = T)
# First loop: read file and tokenize them (in the easiest way)
for (i in 1:length(file_list)) {
# read text
loaded_file <- readLines(file_list[i], warn = F)
loaded_file <- paste(loaded_file, collapse = "\n")
# tokenize
tm_corpus[[i]] <- unlist(strsplit(loaded_file, "\\W"))
tm_corpus[[i]] <- tm_corpus[[i]][which(tm_corpus[[i]] != "")]
# then add correct names to the different texts in the list
# (we can re-use the names saved in the list_files variable, by deleting the "corpus/" at the beginning)
names(tm_corpus)[i] <- gsub(pattern = "corpus_all/|.txt", replacement = "", x = file_list[i])
# print progress
print(i)
}
# visualize the output
tm_corpus[[1]]
# Second loop to generate final files
counter <- 1
for (i in 1:length(tm_corpus)) {
# work on single text
tokenized_text <- tm_corpus[[i]]
# get total length
len_limit <- length(tokenized_text)
# use total length to get the number of times you can split it
split_dim <- trunc(len_limit/len_split)
# then do the actual splitting
tokenized_text_split <- split(tokenized_text, ceiling(seq_along(tokenized_text)/len_split))
# # last part will be shorter than the set length, so better merge it with the previous one
# tokenized_text_split[[length(tokenized_text_split)-1]] <- c(tokenized_text_split[[length(tokenized_text_split)-1]], tokenized_text_split[[length(tokenized_text_split)]])
# tokenized_text_split <- tokenized_text_split[-length(tokenized_text_split)]
# then collapse back the split texts into a single string
tokenized_text_split <- unlist(lapply(tokenized_text_split, function(x) paste(x, collapse = " ")))
# finally we perform a loop on the split texts to incrementally save all in just one variable
for (n in 1:length(tokenized_text_split)){
# put to lowercase
my_texts[counter] <- tolower(tokenized_text_split[n])
# assign names
names(my_texts)[counter] <- paste(names(tm_corpus)[i], n, sep = "_")
# increase the counter (started as 1 from outside the loop)
counter <- counter + 1
}
print(i)
}
# visualize the output
my_texts[1]
library(mallet)
# preparation of texts for topic model
text.instances <-
mallet.import(text.array = my_texts,
stoplist = as.character(stopwords::stopwords("es")),
id.array = names(my_texts))
# define all variables (better not to change alpha and beta)
topic.model <- MalletLDA(num.topics = num_topics, alpha.sum = 1, beta = 0.1)
# load documents for topic modeling
topic.model$loadDocuments(text.instances)
# create the topic models
topic.model$setAlphaOptimization(20, 50) # this is for optimization
topic.model$train(n_iterations) # number of iterations defined at the beginning
# calculate topic per document
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
# calculate topic per words
topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)
# use a loop to visualize the top words per topic in a table
top_words <- data.frame()
firstwords <- character()
for (i in 1:num_topics) {
words.per.topic <- mallet.top.words(topic.model, word.weights = topic.words[i,], num.top.words = 20)
words.per.topic$topic <- i
top_words <- rbind(top_words, words.per.topic)
firstwords[i] <- paste(words.per.topic$words[1:5], collapse = " ")
}
# visualize the table
View(top_words)
# visualize the first five words per topic
names(firstwords) <- paste("Topic", 1:length(firstwords))
firstwords
# first assign names that correspond to:
# the first five words of the topics
colnames(doc.topics) <- firstwords
# the titles of the documents
rownames(doc.topics) <- names(my_texts) # to make them look better, remove "corpus" from the names
# visualize an heatmap and save it to a file
png(filename = "heatmap.png", width = 4000, height = 4000)
heatmap(doc.topics, margins = c(25,25), cexRow = 2, cexCol = 2)
dev.off()
heatmap(doc.topics, margins = c(25,25), cexRow = 2, cexCol = 2)
# visualize an heatmap and save it to a file
png(filename = "heatmap.png")
heatmap(doc.topics, margins = c(25,25), cexRow = 2, cexCol = 2)
# visualize the table
View(top_words)
library(tidyverse)
library(ggplot2)
library(readr)
library(data.table)
library(syuzhet)
library(tm)
library(tidytext)
library(sjPlot)
library(wordcloud)
library(textdata)
library(readtext)
options(stringsAsFactors = F, # do not convert to factor upon loading
scipen = 999, # do not convert numbers to e-values
max.print = 200, # stop printing after 200 values
warn = -1) # as many warnings in R are useless (and annoying), you might want to disable them
theme_set(theme_sjplot2()) # set default ggplot theme to light
fs = 12 # default plot font size
corpus_source <- readtext("corpus_all/*.txt", encoding = "UTF-8") %>%
mutate(text = gsub("\\s+"," ", text))  %>%
as_tibble() %>%
mutate(doc_id = stringr::str_remove(doc_id, ".txt")) %>%
left_join(metadata_all, by = "doc_id") %>%
as_tibble()
# load required objects
load("corpus_all/metadata_all.rda")
corpus_source <- readtext("corpus_all/*.txt", encoding = "UTF-8") %>%
mutate(text = gsub("\\s+"," ", text))  %>%
as_tibble() %>%
mutate(doc_id = stringr::str_remove(doc_id, ".txt")) %>%
left_join(metadata_all, by = "doc_id") %>%
as_tibble()
head(corpus_source)
corpus_sentences <- corpus_source %>%
unnest_lines(input = text, output = sentence, drop = T, to_lower = F) %>%
group_by(doc_id) %>%
mutate(sentence_id = seq_along(sentence)) %>%
ungroup() %>%
mutate(unique_sentence_id = seq_along(sentence))
corpus_tokens <- corpus_sentences %>%
unnest_tokens(input = sentence, output = token, drop = T, to_lower = T) %>%
# we turn all to lowercase
group_by(doc_id, sentence_id) %>%
mutate(token_id = seq_along(token)) %>%
ungroup() %>%
mutate(unique_token_id = seq_along(token))
head(corpus_tokens)
corpus_tokens %>%
group_by(song_name, token) %>%
anti_join(as_tibble(stopwords("es")), by = c("token" = "value")) %>% # delete stopwords
count() %>% # summarize count per token per title
arrange(desc(n)) %>% # highest freq on top
group_by(title) %>% #
mutate(top = seq_along(token)) %>% # identify rank within group
filter(top <= 15) %>% # retain top 15 frequent words
# create barplot
ggplot(aes(x = -top, fill = title)) +
geom_bar(aes(y = n), stat = 'identity', col = 'black') +
# make sure words are printed either in or next to bar
geom_text(aes(y = ifelse(n > max(n) / 2, max(n) / 50, n + max(n) / 50),
label = token), size = fs/3, hjust = "left") +
theme(legend.position = 'none', # get rid of legend
text = element_text(size = fs), # determine fs
axis.text.x = element_text(angle = 45, hjust = 1, size = fs/1.5), # rotate x text
axis.ticks.y = element_blank(), # remove y ticks
axis.text.y = element_blank()) + # remove y text
labs(y = "token count", x = "", # add labels
title = "Most frequent words throughout the songs") +
facet_grid(. ~ title) + # separate plot for each title
coord_flip() + # flip axes
scale_fill_sjplot()
corpus_tokens %>%
group_by(song_name, token) %>%
anti_join(as_tibble(stopwords("es")), by = c("token" = "value")) %>% # delete stopwords
count() %>% # summarize count per token per title
arrange(desc(n)) %>% # highest freq on top
group_by(song_name) %>% #
mutate(top = seq_along(token)) %>% # identify rank within group
filter(top <= 15) %>% # retain top 15 frequent words
# create barplot
ggplot(aes(x = -top, fill = song_name)) +
geom_bar(aes(y = n), stat = 'identity', col = 'black') +
# make sure words are printed either in or next to bar
geom_text(aes(y = ifelse(n > max(n) / 2, max(n) / 50, n + max(n) / 50),
label = token), size = fs/3, hjust = "left") +
theme(legend.position = 'none', # get rid of legend
text = element_text(size = fs), # determine fs
axis.text.x = element_text(angle = 45, hjust = 1, size = fs/1.5), # rotate x text
axis.ticks.y = element_blank(), # remove y ticks
axis.text.y = element_blank()) + # remove y text
labs(y = "token count", x = "", # add labels
title = "Most frequent words throughout the songs") +
facet_grid(. ~ title) + # separate plot for each title
coord_flip() + # flip axes
scale_fill_sjplot()
corpus_tokens %>%
group_by(song_name, token) %>%
anti_join(as_tibble(stopwords("es")), by = c("token" = "value")) %>% # delete stopwords
count() %>% # summarize count per token per title
arrange(desc(n)) %>% # highest freq on top
group_by(song_name) %>% #
mutate(top = seq_along(token)) %>% # identify rank within group
filter(top <= 15) %>% # retain top 15 frequent words
# create barplot
ggplot(aes(x = -top, fill = song_name)) +
geom_bar(aes(y = n), stat = 'identity', col = 'black') +
# make sure words are printed either in or next to bar
geom_text(aes(y = ifelse(n > max(n) / 2, max(n) / 50, n + max(n) / 50),
label = token), size = fs/3, hjust = "left") +
theme(legend.position = 'none', # get rid of legend
text = element_text(size = fs), # determine fs
axis.text.x = element_text(angle = 45, hjust = 1, size = fs/1.5), # rotate x text
axis.ticks.y = element_blank(), # remove y ticks
axis.text.y = element_blank()) + # remove y text
labs(y = "token count", x = "", # add labels
title = "Most frequent words throughout the songs") +
facet_grid(. ~ song_name) + # separate plot for each title
coord_flip() + # flip axes
scale_fill_sjplot()
corpus_source <- readtext("corpus_all/*.txt", encoding = "UTF-8") %>%
mutate(text = gsub("\\s+"," ", text))  %>%
as_tibble() %>%
mutate(doc_id = stringr::str_remove(doc_id, ".txt")) %>%
left_join(metadata_all, by = "doc_id") %>%
group_by(is_male) %>%
sample_n(5) %>%
as_tibble()
View(corpus_source)
corpus_source <- readtext("corpus_all/*.txt", encoding = "UTF-8") %>%
mutate(text = gsub("\\s+"," ", text))  %>%
as_tibble() %>%
mutate(doc_id = stringr::str_remove(doc_id, ".txt")) %>%
left_join(metadata_all, by = "doc_id") %>%
group_by(as_factor(is_male)) %>%
sample_n(5) %>%
as_tibble()
View(metadata_all)
corpus_source <- readtext("corpus_all/*.txt", encoding = "UTF-8") %>%
mutate(text = gsub("\\s+"," ", text))  %>%
mutate(doc_id = stringr::str_remove(doc_id, ".txt")) %>%
left_join(metadata_all, by = "doc_id") %>%
group_by(as_factor(is_male)) %>%
sample_n(5) %>%
as_tibble()
corpus_source <- readtext("corpus_all/*.txt", encoding = "UTF-8") %>%
mutate(text = gsub("\\s+"," ", text))  %>%
mutate(doc_id = stringr::str_remove(doc_id, ".txt")) %>%
left_join(metadata_all, by = "doc_id")
View(corpus_source)
corpus_source <- readtext("corpus_all/*.txt", encoding = "UTF-8") %>%
mutate(doc_id = stringr::str_remove(doc_id, ".txt")) %>%
left_join(metadata_all, by = "doc_id") %>%
group_by(as_factor(is_male)) %>%
sample_n(5) %>%
as_tibble()
corpus_source <- readtext("corpus_all/*.txt", encoding = "UTF-8") %>%
mutate(doc_id = stringr::str_remove(doc_id, ".txt")) %>%
left_join(metadata_all, by = "doc_id")
View(corpus_source)
setwd("corpus_all")
# 2. Call stylo's main function
stylo()
# 1. Install and call the package
#install.packages("stylo")
library(stylo)
# 2. Call stylo's main function
stylo()
